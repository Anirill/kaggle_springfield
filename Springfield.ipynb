{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5o6tUfAH24S5",
        "outputId": "e8eaa21a-6f99-489a-b18b-5138138fc569"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA Available\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from os import listdir\n",
        "from os.path import isfile, isdir, join\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "if USE_CUDA:\n",
        "    print(\"CUDA Available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NO1TtaL24S-"
      },
      "outputs": [],
      "source": [
        "TRAIN_DATA = './train/simpsons_dataset/'\n",
        "TEST_DATA = './testset/testset/'\n",
        "SORTED = './sorted/'\n",
        "TRAIN_DIVISION = True\n",
        "\n",
        "IMG_SIZE = 224\n",
        "\n",
        "MODEL_PATH = './model_b0_t.pth'\n",
        "best_eval = None"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "kaDHecnn24S_"
      },
      "source": [
        "Классов в train достаточно много и они дизбалансные(3 изображения в самом маленьком и 2246 в самом большом). Это создаст проблемы при обучении и смещение в сторону крупных классов. Можно просто выкинуть самые мелкие классы, считая их долю незначительной в тестовой выборке. Можно сгенерировать новые данные из существующих, с помощью поворотов, отражений, и т.д и выровнять количество. Ещё один вариант - поступить проще, можно это частично обойти создав датасет, равномерно выдавая из него данные по классам, и случайно аугментируя для повышения разнообразия выборки.\n",
        "    \n",
        "Размер изображений train и test различается, большинство изображений в test меньшего размера, а модель применять к ним мы будем одну и ту же, т.о. нужно расширить домен train чтобы приблизить его к test, при обучении я добавим отдельный resize, который увеличивает изображение, и добавим crop_center, чтобы сделать изображение из обучающей выборки похожим на растянутое изображение из test'а, применять их будем с некоторой вероятностью.     \n",
        "    \n",
        "На первый взгляд показалось, что в test представлены не все классы train, также хотелось бы узнать можно ли выбросить классы, которые мало представлены без большой потери качества. Но в большом количестве файлов сложно это отследить поэтому, нужно придумать способ визуального контроля, идея - рапределить test по папкам также как это сделано в train. Тогда будет видно, если какие-то папки останутся пустыми, или какие-то изображения будут не в сввоих папках. (потренировав модель пару эпох и сделав сортировку изображений по классам оказалось, что больше половины не представлены в test, и для решения датасета их можно выбросить, \"по стечению обстоятельств\" именно эти классы оказались самыми ненаполненными)Как итог - осталось 20 классов из 42.\n",
        "\n",
        "В качестве основы для модели взял предобученную на датасете imagenet сеть efficientnet, для неё нужно отскейлить изображение в размер 224, а для того, чтобы можно было работать с батчами все изобрадения будем приводить к квадратной форме (224, 224). Чтобы модель работала правильно помимо геометрических преобразований нужно нормализовать данные по стандарту imagenet.\n",
        "    \n",
        "Абсолютно одинаковых изображений в train'е нет, но есть близкие, вероятно последовательные кадры. А для правильного обучения модели важно разнообразие. Это также помогут решить геометрические преобразования. В test есть одинаковые изображения, но это не имеет значения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6CHS7iX24TA"
      },
      "outputs": [],
      "source": [
        "RESIZE = transforms.Resize(int(IMG_SIZE*1.5))\n",
        "CENTER_CROP = transforms.CenterCrop((IMG_SIZE, IMG_SIZE))\n",
        "IMAGE_TRANSFORMS = transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.RandomRotation(20),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.3),\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f8IiWT324TB"
      },
      "outputs": [],
      "source": [
        "def get_list_from_dir(path, full=False, directory=False):\n",
        "    ''' Returns list of file names and full paths in directory, \n",
        "        if full==False, then list of names only.\n",
        "    '''\n",
        "    if directory:\n",
        "        if full:\n",
        "            return [os.path.join(path, f) for f in listdir(path) if isdir(join(path, f))]\n",
        "        else:\n",
        "            return [f for f in listdir(path) if isdir(join(path, f))]\n",
        "    if full:\n",
        "        return [os.path.join(path, f) for f in listdir(path) if isfile(join(path, f))]\n",
        "    else:\n",
        "        return [f for f in listdir(path) if isfile(join(path, f))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBV_DukA24TC"
      },
      "outputs": [],
      "source": [
        "cat_list = get_list_from_dir(TRAIN_DATA, directory=True)\n",
        "NUM_CLASSES = len(cat_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqY2rJFI24TC"
      },
      "outputs": [],
      "source": [
        "cat_to_id = dict(zip(cat_list, list(range(len(cat_list)))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfHIPk7r24TD"
      },
      "outputs": [],
      "source": [
        "counts = []\n",
        "for cat in cat_list:\n",
        "    counts.append(len(get_list_from_dir(os.path.join(TRAIN_DATA, cat))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJjJZb4U24TE",
        "outputId": "b2036e66-48d1-4dba-899a-63b691d996da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counts: [246, 310, 358, 457, 469, 498, 623, 877, 913, 986, 1079, 1193, 1194, 1206, 1291, 1342, 1354, 1452, 1454, 2246]\n"
          ]
        }
      ],
      "source": [
        "counts.sort()\n",
        "print(f'Counts: {counts}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgogC6wX24TE"
      },
      "outputs": [],
      "source": [
        "class SimpsonsDataset(torch.utils.data.Dataset):\n",
        "    ''' Create dataset to format data, to use in standard Dataloader    \n",
        "    '''\n",
        "    def __init__(self, \n",
        "                 cat_list,\n",
        "#                  image_size=IMG_SIZE,                  \n",
        "                 train_path=TRAIN_DATA,\n",
        "                 test_path=TEST_DATA,\n",
        "                 train=True, \n",
        "                 train_fraction=0.85\n",
        "                ):\n",
        "        self.path_images = train_path\n",
        "        self.cat_list = cat_list\n",
        "        self.files = []\n",
        "        self.ids = [] # indices of filenames\n",
        "        self.id = 0\n",
        "        for i, cat in enumerate(cat_list):\n",
        "            files = get_list_from_dir(os.path.join(self.path_images, cat))\n",
        "            self.files.append(files)\n",
        "            self.ids.append(list(range(len(files))))           \n",
        "        random.seed(0)\n",
        "        \n",
        "        self._indices = [[] for _ in range(len(self.ids))]  # indices of split\n",
        "        self._len = 0\n",
        "        for i in range(len(self.ids)):             \n",
        "            random.shuffle(self.ids[i])\n",
        "            train_size = int(len(self.ids[i]) * train_fraction)\n",
        "            if train:\n",
        "                self._len += train_size\n",
        "            else:\n",
        "                self._len += (len(self.ids[i]) - train_size)\n",
        "            self._indices[i] = self.ids[i][:train_size] if train else self.ids[i][train_size:]        \n",
        "   \n",
        "    def get_next_id(self):\n",
        "        self.id += 1\n",
        "        if self.id >= len(self.ids):\n",
        "            self.id = 0\n",
        "        return self.id\n",
        "    \n",
        "    def get_one_hot(self, id):\n",
        "        ''' One-hot encode for mask\n",
        "        '''\n",
        "        out = [0 for _ in range(len(self.ids))]\n",
        "        out[id] = 1\n",
        "        return torch.tensor(out)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self._len\n",
        "    \n",
        "    def tfs(self, image, rnd=True):\n",
        "        if rnd:\n",
        "            if np.random.random() < 0.45:\n",
        "                image = RESIZE(image)\n",
        "            if np.random.random() < 0.45:\n",
        "                image = CENTER_CROP(image)\n",
        "        return IMAGE_TRANSFORMS(image)\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        ''' Here choosing random class first, to negate class disbalance.\n",
        "            Then choosing image from class and transform it\n",
        "        '''\n",
        "        idx = np.random.randint(0, len(self.ids))\n",
        "        while len(self._indices[idx]) == 0:  # if come class is empty\n",
        "            idx = np.random.randint(0, len(self.ids))\n",
        "        file_id = np.random.randint(0, len(self._indices[idx]))\n",
        "        \n",
        "        name = self.files[idx][self._indices[idx][file_id]]\n",
        "        \n",
        "        img_name = os.path.join(self.path_images, self.cat_list[idx], name)        \n",
        "        image = Image.open(img_name)\n",
        "        image = self.tfs(image)       \n",
        "        \n",
        "        return image, self.get_one_hot(idx) #, img_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef-Umz6K24TF"
      },
      "outputs": [],
      "source": [
        "trainset = SimpsonsDataset(cat_list, train=True)\n",
        "valset = SimpsonsDataset(cat_list, train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LrQlB-j24TF",
        "outputId": "e8430163-fc93-468d-ba40-57817f4c5006"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train part: 16608\n",
            "Test part : 2940\n"
          ]
        }
      ],
      "source": [
        "print(f'Train part: {len(trainset)}')\n",
        "print(f'Test part : {len(valset)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRGxKKFv24TG"
      },
      "outputs": [],
      "source": [
        "image, mask = trainset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6Y4_cXu24TG"
      },
      "outputs": [],
      "source": [
        "# pretrained efficientnet model with b0 version, can get higher version for higher quality\n",
        "model = EfficientNet.from_name('efficientnet-b0')\n",
        "# replacing output layer with new for our class quantity\n",
        "model._fc = torch.nn.Linear(in_features=model._fc.in_features, out_features=NUM_CLASSES, bias=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQum1Yd524TG"
      },
      "outputs": [],
      "source": [
        "def make_optimizer(model, lr=3e-4):\n",
        "    return torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyIByKhB24TH"
      },
      "outputs": [],
      "source": [
        "loss = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArCiUjqu24TH"
      },
      "outputs": [],
      "source": [
        "if USE_CUDA:\n",
        "    model.cuda()\n",
        "    loss.cuda()\n",
        "optimizer = make_optimizer(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrDgkYBZ24TH"
      },
      "outputs": [],
      "source": [
        "def train_loop(model, optimizer, loss, train_loader, \n",
        "               n_iter, lr_scheduler=None, plot=None, \n",
        "               plot_kwargs={}, use_cuda=False, plot_steps=10):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for i, (images, masks) in enumerate(train_loader):\n",
        "        if i == n_iter:\n",
        "            break\n",
        "        if use_cuda:\n",
        "            images = images.cuda()\n",
        "            masks = masks.cuda()\n",
        "        predicted = model(images)\n",
        "        loss_value = loss(predicted, masks.float())\n",
        "        \n",
        "        optimizer.zero_grad()        \n",
        "        loss_value.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        losses.append(loss_value.item())\n",
        "        if i % 10 == 0:\n",
        "            print(\"Step {} / {}, loss: {:.4f}, learning rate: {:.4f}\\r\".format(i, n_iter, loss_value.item(), optimizer.param_groups[0][\"lr\"]), end=\"\")\n",
        "    print(\" \" * 50 + \"\\r\", end=\"\")\n",
        "    print(\"Train loss: {:.4f}, learning rate: {:.5f}\".format(np.mean(losses[-plot_steps:]), optimizer.param_groups[0][\"lr\"]))\n",
        "    return np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4OCdvb024TH"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, loss, testset, batch_size,\n",
        "               use_cuda=False,\n",
        "               num_workers=1):\n",
        "    model.eval()\n",
        "#     clear_output()\n",
        "    kwargs = {}\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        testset, \n",
        "        batch_size=batch_size,\n",
        "        num_workers=0)\n",
        "    \n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for images, masks in test_loader:\n",
        "            if use_cuda:\n",
        "                images = images.cuda()\n",
        "                masks = masks.cuda()    \n",
        "            predicted = model(images)\n",
        "\n",
        "            loss_value = loss(predicted, masks.float())            \n",
        "            losses.append(loss_value.item())\n",
        "    test_loss = np.mean(losses)\n",
        "    print(\"Test loss:\", test_loss)\n",
        "    return test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHVgRmqd24TI"
      },
      "outputs": [],
      "source": [
        "# # this used for cache empty, to choose right batch_size\n",
        "# torch.cuda.empty_cache()\n",
        "# if USE_CUDA:\n",
        "#     model.cuda()\n",
        "#     loss.cuda()\n",
        "# optimizer = make_optimizer(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zthSbeAL24TI"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "n_iters = 2000\n",
        "eval_steps = 1000\n",
        "loss_storage = []\n",
        "train_loss_storage = []\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                    shuffle=True, num_workers=0)\n",
        "# eval_loss = eval_model(model, loss, valset, batch_size, use_cuda=USE_CUDA)\n",
        "# loss_storage.append(eval_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BXpDUxd24TI"
      },
      "outputs": [],
      "source": [
        "epoch = 0\n",
        "if best_eval is None:\n",
        "    best_eval = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URXV20kh24TI",
        "outputId": "4efe8d0e-aa36-4203-9b4d-d912de5d6d36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.291168636163765\n",
            "Epoch: 1\n",
            "Wall time: 15min 2s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "num_epochs = 1\n",
        "while epoch < num_epochs:\n",
        "    i = 0\n",
        "    while i < n_iters:    \n",
        "        train_steps = min(eval_steps, n_iters - i)\n",
        "        train_loss = train_loop(model, optimizer, loss, train_loader, train_steps,\n",
        "                   lr_scheduler=None,\n",
        "                   use_cuda=USE_CUDA)\n",
        "        i += train_steps\n",
        "    eval_loss = eval_model(model, loss, valset, batch_size,\n",
        "                           use_cuda=USE_CUDA)\n",
        "    if eval_loss < best_eval:\n",
        "        best_eval = eval_loss\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': eval_loss,\n",
        "            }, MODEL_PATH)\n",
        "        print('Saving...')\n",
        "    print(f'Epoch: {epoch+1}')\n",
        "    loss_storage.append(eval_loss)\n",
        "    train_loss_storage.append(train_loss)\n",
        "    epoch = epoch + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crOV--BA24TJ",
        "outputId": "ebf6c157-1f48-4716-f9e7-223c99b18fa5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1.583194928648679,\n",
              " 0.8027988547540229,\n",
              " 0.6389938513140964,\n",
              " 0.5546677119463034,\n",
              " 0.4849191670098504,\n",
              " 0.36140497557252,\n",
              " 0.3766913807434637,\n",
              " 0.30669559792453505,\n",
              " 0.31727622835213604,\n",
              " 0.31575080315119325,\n",
              " 0.28149506299118715,\n",
              " 0.2939396672996024]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss_storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "DZAHCso424TJ",
        "outputId": "f20ccfff-4b26-4910-9c80-8ca850c5dda7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load best evaluated model\n",
        "model.load_state_dict(torch.load(MODEL_PATH)['model_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEkqaVz524TJ"
      },
      "outputs": [],
      "source": [
        "# getting all image names from test dir\n",
        "name_list = get_list_from_dir(TEST_DATA, full=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rubVkW8Q24TK"
      },
      "outputs": [],
      "source": [
        "UPSCALE = transforms.Resize((IMG_SIZE, IMG_SIZE)) # for display only\n",
        "TEST_TRANSFORMS = transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbqDU4pw24TK"
      },
      "outputs": [],
      "source": [
        "def sort_images(model, name_list, cat_list, submission, verbose=False):\n",
        "    ''' Real validation is unavailable, \n",
        "        so we sort images to class directories for visual validation,\n",
        "        also creating submission here.\n",
        "    '''\n",
        "    submission = {\"Id\": [], \"Expected\": []}\n",
        "    model.eval()\n",
        "    for name in name_list:\n",
        "        submission['Id'].append(name)\n",
        "        img_name = os.path.join(TEST_DATA, name)\n",
        "        img = Image.open(img_name)        \n",
        "        image = TEST_TRANSFORMS(img) \n",
        "        with torch.no_grad():\n",
        "            if USE_CUDA:\n",
        "                predicted = model(image.unsqueeze(0).cuda())\n",
        "            else:\n",
        "                predicted = model(image.unsqueeze(0))\n",
        "\n",
        "        probs = torch.softmax(predicted, dim=-1)\n",
        "        image_id = torch.argmax(probs).item()\n",
        "        submission['Expected'].append(cat_list[image_id])\n",
        "        if verbose:\n",
        "            ima = UPSCALE(img)\n",
        "            display(ima)\n",
        "            print(predicted)\n",
        "            print(probs)\n",
        "            print(image_id)\n",
        "            print(cat_list[image_id])\n",
        "        if not os.path.exists(os.path.join(SORTED, cat_list[image_id])):\n",
        "            os.makedirs(os.path.join(SORTED, cat_list[image_id]))\n",
        "        save_path = os.path.join(SORTED, cat_list[image_id], name)\n",
        "        with open(save_path, 'w') as f:            \n",
        "            img.save(f)\n",
        "    pd.DataFrame(submission).to_csv(\"simpsons_submission.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "QZwGhUgr24TK"
      },
      "outputs": [],
      "source": [
        "sort_images(model, name_list, cat_list, submission, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIzJj7_B24TK"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Springfield.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}